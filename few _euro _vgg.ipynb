{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3816224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2131eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.1307,),(0.3081,))\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485,0.456,0.406],\n",
    "        std=[0.229,0.224,0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "fulldata=datasets.EuroSAT(\n",
    "    download=True,\n",
    "    root='./data',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "train_classes = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "test_classes = [8, 9]\n",
    "\n",
    "\n",
    "def filter_dataset_by_class(dataset, class_list):\n",
    "    indices = [i for i, target in enumerate(dataset.targets) if target in class_list]\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "\n",
    "data = filter_dataset_by_class(fulldata, train_classes)\n",
    "test_data = filter_dataset_by_class(fulldata, test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a8fdb5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset EuroSAT\n",
       "    Number of datapoints: 27000\n",
       "    Root location: ./data\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fulldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60fcb09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9c8d330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stark/Deep Learning/temp.env/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "vgg=models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "657f93e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = vgg.features\n",
    "        self.avgpool = vgg.avgpool\n",
    "        \n",
    "        self.classifier = nn.Sequential(*list(vgg.classifier.children())[:-1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "999b8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "class ResNet18Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1]) # Remove final FC layer\n",
    "        self.embedding = nn.Linear(512, 10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        return x  # Output: 512-dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b946a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Layer-wise requires_grad ===\n",
      "features.0.weight: ❌ Frozen\n",
      "features.1.weight: ❌ Frozen\n",
      "features.1.bias: ❌ Frozen\n",
      "features.4.0.conv1.weight: ❌ Frozen\n",
      "features.4.0.bn1.weight: ❌ Frozen\n",
      "features.4.0.bn1.bias: ❌ Frozen\n",
      "features.4.0.conv2.weight: ❌ Frozen\n",
      "features.4.0.bn2.weight: ❌ Frozen\n",
      "features.4.0.bn2.bias: ❌ Frozen\n",
      "features.4.1.conv1.weight: ❌ Frozen\n",
      "features.4.1.bn1.weight: ❌ Frozen\n",
      "features.4.1.bn1.bias: ❌ Frozen\n",
      "features.4.1.conv2.weight: ❌ Frozen\n",
      "features.4.1.bn2.weight: ❌ Frozen\n",
      "features.4.1.bn2.bias: ❌ Frozen\n",
      "features.5.0.conv1.weight: ❌ Frozen\n",
      "features.5.0.bn1.weight: ❌ Frozen\n",
      "features.5.0.bn1.bias: ❌ Frozen\n",
      "features.5.0.conv2.weight: ❌ Frozen\n",
      "features.5.0.bn2.weight: ❌ Frozen\n",
      "features.5.0.bn2.bias: ❌ Frozen\n",
      "features.5.0.downsample.0.weight: ❌ Frozen\n",
      "features.5.0.downsample.1.weight: ❌ Frozen\n",
      "features.5.0.downsample.1.bias: ❌ Frozen\n",
      "features.5.1.conv1.weight: ❌ Frozen\n",
      "features.5.1.bn1.weight: ❌ Frozen\n",
      "features.5.1.bn1.bias: ❌ Frozen\n",
      "features.5.1.conv2.weight: ❌ Frozen\n",
      "features.5.1.bn2.weight: ❌ Frozen\n",
      "features.5.1.bn2.bias: ❌ Frozen\n",
      "features.6.0.conv1.weight: ❌ Frozen\n",
      "features.6.0.bn1.weight: ❌ Frozen\n",
      "features.6.0.bn1.bias: ❌ Frozen\n",
      "features.6.0.conv2.weight: ❌ Frozen\n",
      "features.6.0.bn2.weight: ❌ Frozen\n",
      "features.6.0.bn2.bias: ❌ Frozen\n",
      "features.6.0.downsample.0.weight: ❌ Frozen\n",
      "features.6.0.downsample.1.weight: ❌ Frozen\n",
      "features.6.0.downsample.1.bias: ❌ Frozen\n",
      "features.6.1.conv1.weight: ❌ Frozen\n",
      "features.6.1.bn1.weight: ❌ Frozen\n",
      "features.6.1.bn1.bias: ❌ Frozen\n",
      "features.6.1.conv2.weight: ❌ Frozen\n",
      "features.6.1.bn2.weight: ❌ Frozen\n",
      "features.6.1.bn2.bias: ❌ Frozen\n",
      "features.7.0.conv1.weight: ❌ Frozen\n",
      "features.7.0.bn1.weight: ❌ Frozen\n",
      "features.7.0.bn1.bias: ❌ Frozen\n",
      "features.7.0.conv2.weight: ❌ Frozen\n",
      "features.7.0.bn2.weight: ❌ Frozen\n",
      "features.7.0.bn2.bias: ❌ Frozen\n",
      "features.7.0.downsample.0.weight: ❌ Frozen\n",
      "features.7.0.downsample.1.weight: ❌ Frozen\n",
      "features.7.0.downsample.1.bias: ❌ Frozen\n",
      "features.7.1.conv1.weight: ❌ Frozen\n",
      "features.7.1.bn1.weight: ❌ Frozen\n",
      "features.7.1.bn1.bias: ❌ Frozen\n",
      "features.7.1.conv2.weight: ❌ Frozen\n",
      "features.7.1.bn2.weight: ❌ Frozen\n",
      "features.7.1.bn2.bias: ❌ Frozen\n",
      "embedding.weight: ✅ Trainable\n",
      "embedding.bias: ✅ Trainable\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,181,642\n",
      "Trainable params: 11,181,642\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 42.65\n",
      "Estimated Total Size (MB): 106.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = ResNet18Embedding().to(device)\n",
    "\n",
    "# # Freeze base layers\n",
    "# for param in model.features.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Show what’s frozen vs trainable\n",
    "# print(\"\\n=== Layer-wise requires_grad ===\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     status = \"✅ Trainable\" if param.requires_grad else \"❌ Frozen\"\n",
    "#     print(f\"{name}: {status}\")\n",
    "\n",
    "\n",
    "# # Summary (if needed)\n",
    "# from torchsummary import summary\n",
    "# summary(model, input_size=(3, 224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97649e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "class ResNet50Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = resnet50(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])  # Remove final FC layer\n",
    "        self.embedding = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        return x  # Output: 2048-dimensional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5caf0aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import densenet121\n",
    "\n",
    "class DenseNet121Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        densenet = densenet121(pretrained=True)\n",
    "        self.features = densenet.features  # Convolutional backbone\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Match ResNet pooling\n",
    "        self.embedding = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        return x  # Output: 1024-dimensional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b08aec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1=nn.Conv2d(3,16,kernel_size=3,padding=1)\n",
    "        self.conv2=nn.Conv2d(16,32,kernel_size=3,padding=1)\n",
    "        self.conv3=nn.Conv2d(32,64,kernel_size=3,padding=1)\n",
    "        self.conv4=nn.Conv2d(64,128,kernel_size=3,padding=1)\n",
    "\n",
    "        self.flat=nn.Flatten()\n",
    "\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.ReLU=nn.ReLU()\n",
    "\n",
    "        self.full=nn.Linear(128*4*4,512)\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.ReLU(x)\n",
    "        x=self.pool(x)\n",
    "\n",
    "        x=self.conv2(x)\n",
    "        x=self.ReLU(x)\n",
    "        x=self.pool(x)\n",
    "\n",
    "        x=self.conv3(x)\n",
    "        x=self.ReLU(x)\n",
    "        x=self.pool(x)\n",
    "\n",
    "        x=self.conv4(x)\n",
    "        x=self.ReLU(x)\n",
    "        x=self.pool(x)\n",
    "\n",
    "        x=self.flat(x)\n",
    "\n",
    "        final=self.full(x)\n",
    "\n",
    "        return final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb8ab03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class fewshot_dataset(Dataset):\n",
    "    def __init__(self,data,way,shot,query,episodes):\n",
    "        self.data=data\n",
    "        self.way=way\n",
    "        self.shot=shot\n",
    "        self.query=query\n",
    "        self.episodes=episodes\n",
    "\n",
    "        self.class_to_indices=self._build_class_index()\n",
    "    \n",
    "    def _build_class_index(self):\n",
    "        class_index={}\n",
    "        for idx, (x,y) in enumerate(self.data):\n",
    "            if y not in class_index:\n",
    "                class_index[y]=[]\n",
    "            class_index[y].append(idx)\n",
    "        return class_index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.episodes\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        selected_classes=random.sample(list(self.class_to_indices.keys()), self.way)\n",
    "\n",
    "        support_images=[]\n",
    "        support_labels=[]\n",
    "        query_images=[]\n",
    "        query_labels=[]\n",
    "\n",
    "\n",
    "        label_map={class_name: i for i, class_name in enumerate(selected_classes)}\n",
    "        \n",
    "        for class_name in selected_classes:\n",
    "            all_indices=self.class_to_indices[class_name]\n",
    "            selected_indices=random.sample(all_indices,self.shot+self.query)\n",
    "\n",
    "            support_idx=selected_indices[:self.shot]\n",
    "            query_idx=selected_indices[self.shot:]\n",
    "\n",
    "            for idx in support_idx:\n",
    "                image,_=self.data[idx]\n",
    "                support_images.append(image)\n",
    "                support_labels.append(torch.tensor(label_map[class_name]))\n",
    "            \n",
    "            for idx in query_idx:\n",
    "                image,_=self.data[idx]\n",
    "                query_images.append(image)\n",
    "                query_labels.append(torch.tensor(label_map[class_name]))\n",
    "        return(\n",
    "             torch.stack(support_images),\n",
    "             torch.stack(support_labels),\n",
    "             torch.stack(query_images),\n",
    "             torch.stack(query_labels)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "220ed906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prototypes(support_embeddings,support_labels,way):\n",
    "    embedding_dimensions=support_embeddings.size(-1)\n",
    "    prototypes=torch.zeros(way,embedding_dimensions).to(support_embeddings.device)\n",
    "\n",
    "    for c in range(way):\n",
    "        class_mask=(support_labels==c)\n",
    "        class_embeddings=support_embeddings[class_mask]\n",
    "        prototypes[c]=class_embeddings.mean(dim=0)\n",
    "    return prototypes\n",
    "\n",
    "def classify_queries(prototypes,query_embeddings):\n",
    "    n_query=query_embeddings.size(0)\n",
    "    way=prototypes.size(0)\n",
    "    \n",
    "    query_exp=query_embeddings.unsqueeze(1).expand(n_query,way,-1)  ##important\n",
    "    prototypes_exp=prototypes.unsqueeze(0).expand(n_query,way,-1)  ##impotant\n",
    "\n",
    "    distances=torch.sum((query_exp-prototypes_exp)**2,dim=2)\n",
    "\n",
    "    logits=-distances\n",
    "    return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "794df537",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_dataset=fewshot_dataset(\n",
    "    data=data,\n",
    "    way=2,\n",
    "    shot=1,\n",
    "    query=2,\n",
    "    episodes=100\n",
    ")\n",
    "\n",
    "dataloader=DataLoader(few_dataset,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ef313264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b55556f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2630888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=CNN()\n",
    "model=VGGEmbedding()\n",
    "##Need to ask\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad=False\n",
    "model.classifier[3]=nn.Linear(model.classifier[3].in_features,10)\n",
    "device=torch.device(\"cuda\")\n",
    "model=model.to(device)\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(),lr=1e-4)\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d062dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/20] --> Loss:1.2065574595332145 and Accuracy:50.5\n",
      "Epoch:[2/20] --> Loss:0.6932703733444214 and Accuracy:50.0\n",
      "Epoch:[3/20] --> Loss:0.6931528782844544 and Accuracy:50.0\n",
      "Epoch:[4/20] --> Loss:0.6931471824645996 and Accuracy:50.0\n",
      "Epoch:[5/20] --> Loss:0.6931471884250641 and Accuracy:50.0\n",
      "Epoch:[6/20] --> Loss:0.6934351646900176 and Accuracy:50.0\n",
      "Epoch:[7/20] --> Loss:0.6931644684076309 and Accuracy:50.0\n",
      "Epoch:[8/20] --> Loss:0.693147297501564 and Accuracy:50.0\n",
      "Epoch:[9/20] --> Loss:0.6931471824645996 and Accuracy:50.0\n",
      "Epoch:[10/20] --> Loss:0.6931471824645996 and Accuracy:50.0\n",
      "Epoch:[11/20] --> Loss:0.6931471824645996 and Accuracy:50.0\n",
      "Epoch:[12/20] --> Loss:0.6931471824645996 and Accuracy:50.0\n",
      "Epoch:[13/20] --> Loss:0.6931471824645996 and Accuracy:50.0\n",
      "Epoch:[14/20] --> Loss:0.6932056248188019 and Accuracy:50.0\n",
      "Epoch:[15/20] --> Loss:0.6931471824645996 and Accuracy:50.0\n",
      "Epoch:[16/20] --> Loss:0.6931471824645996 and Accuracy:50.0\n",
      "Epoch:[17/20] --> Loss:0.6931471824645996 and Accuracy:50.0\n",
      "Epoch:[18/20] --> Loss:0.693149721622467 and Accuracy:50.0\n",
      "Epoch:[19/20] --> Loss:0.6931471824645996 and Accuracy:50.0\n",
      "Epoch:[20/20] --> Loss:0.6931471824645996 and Accuracy:50.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    total_correct=0\n",
    "    total_queries=0\n",
    "\n",
    "    for episode in dataloader:\n",
    "        support_images,support_labels, query_images, query_labels=episode\n",
    "\n",
    "        support_images=(support_images.squeeze(0)).to(device)\n",
    "        query_images=(query_images.squeeze(0)).to(device)\n",
    "        support_labels=(support_labels.view(-1)).to(device)\n",
    "        query_labels=(query_labels.view(-1)).to(device)\n",
    "\n",
    "        support_embeddings=model(support_images)\n",
    "        query_embeddings=model(query_images)\n",
    "\n",
    "        n_way=torch.unique(support_labels).size(0)\n",
    "        prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "\n",
    "        logits=classify_queries(prototypes, query_embeddings)\n",
    "        loss=loss_fn(logits, query_labels)\n",
    "        total_loss+=loss.item()\n",
    "\n",
    "        preds=torch.argmax(logits,dim=1)\n",
    "        correct=(preds==query_labels).sum().item()\n",
    "        total_correct+=correct\n",
    "        total_queries+=query_labels.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # del support_images, query_images, support_labels, query_labels\n",
    "        # del support_embeddings, query_embeddings, prototypes, logits, preds, loss\n",
    "        # torch.cuda.empty_cache()\n",
    "        # import gc\n",
    "        # gc.collect()\n",
    "\n",
    "    avg_loss=total_loss/len(dataloader)\n",
    "    acc=total_correct/total_queries *100\n",
    "\n",
    "    print(f\"Epoch:[{epoch+1}/{epochs}] --> Loss:{avg_loss} and Accuracy:{acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67857aa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VGGEmbedding' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msummary\u001b[49m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Deep Learning/temp.env/lib/python3.13/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'VGGEmbedding' object has no attribute 'summary'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a415a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_dataset=fewshot_dataset(\n",
    "    data=test_data,\n",
    "    way=2,\n",
    "    shot=1,\n",
    "    query=2,\n",
    "    episodes=100\n",
    ")\n",
    "\n",
    "test_loader=DataLoader(few_dataset,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea11c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-shot Accuracy on Unseen Classes (8 & 9): 50.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_correct = 0\n",
    "total_queries = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for episode in test_loader:\n",
    "        support_images, support_labels, query_images, query_labels = episode\n",
    "\n",
    "        support_images = support_images.squeeze(0).to(device)\n",
    "        query_images = query_images.squeeze(0).to(device)\n",
    "        support_labels = support_labels.view(-1).to(device)\n",
    "        query_labels = query_labels.view(-1).to(device)\n",
    "\n",
    "        support_embeddings = model(support_images)\n",
    "        query_embeddings = model(query_images)\n",
    "\n",
    "        n_way = torch.unique(support_labels).size(0)\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way)\n",
    "\n",
    "        logits = classify_queries(prototypes, query_embeddings)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        total_correct += (preds == query_labels).sum().item()\n",
    "        total_queries += query_labels.size(0)\n",
    "\n",
    "acc = total_correct / total_queries * 100\n",
    "print(f\"1-shot Accuracy on Unseen Classes (8 & 9) on VGGNET: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87169013",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43ma\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#model=CNN()\u001b[39;00m\n\u001b[32m      3\u001b[39m model=ResNet18Embedding()\n",
      "\u001b[31mNameError\u001b[39m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#model=CNN()\n",
    "model=ResNet18Embedding()\n",
    "##Need to ask\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad=False\n",
    "device=torch.device(\"cuda\")\n",
    "model=model.to(device)\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(),lr=1e-4)\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "epochs=20\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    total_correct=0\n",
    "    total_queries=0\n",
    "\n",
    "    for episode in dataloader:\n",
    "        support_images,support_labels, query_images, query_labels=episode\n",
    "\n",
    "        support_images=(support_images.squeeze(0)).to(device)\n",
    "        query_images=(query_images.squeeze(0)).to(device)\n",
    "        support_labels=(support_labels.view(-1)).to(device)\n",
    "        query_labels=(query_labels.view(-1)).to(device)\n",
    "\n",
    "        support_embeddings=model(support_images)\n",
    "        query_embeddings=model(query_images)\n",
    "\n",
    "        n_way=torch.unique(support_labels).size(0)\n",
    "        prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "\n",
    "        logits=classify_queries(prototypes, query_embeddings)\n",
    "        loss=loss_fn(logits, query_labels)\n",
    "        total_loss+=loss.item()\n",
    "\n",
    "        preds=torch.argmax(logits,dim=1)\n",
    "        correct=(preds==query_labels).sum().item()\n",
    "        total_correct+=correct\n",
    "        total_queries+=query_labels.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # del support_images, query_images, support_labels, query_labels\n",
    "        # del support_embeddings, query_embeddings, prototypes, logits, preds, loss\n",
    "        # torch.cuda.empty_cache()\n",
    "        # import gc\n",
    "        # gc.collect()\n",
    "\n",
    "    avg_loss=total_loss/len(dataloader)\n",
    "    acc=total_correct/total_queries *100\n",
    "\n",
    "    print(f\"Epoch:[{epoch+1}/{epochs}] --> Loss:{avg_loss} and Accuracy:{acc}\")\n",
    "\n",
    "few_dataset=fewshot_dataset(\n",
    "    data=test_data,\n",
    "    way=2,\n",
    "    shot=1,\n",
    "    query=2,\n",
    "    episodes=100\n",
    ")\n",
    "\n",
    "test_loader=DataLoader(few_dataset,batch_size=1,shuffle=True)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_queries = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for episode in test_loader:\n",
    "        support_images, support_labels, query_images, query_labels = episode\n",
    "\n",
    "        support_images = support_images.squeeze(0).to(device)\n",
    "        query_images = query_images.squeeze(0).to(device)\n",
    "        support_labels = support_labels.view(-1).to(device)\n",
    "        query_labels = query_labels.view(-1).to(device)\n",
    "\n",
    "        support_embeddings = model(support_images)\n",
    "        query_embeddings = model(query_images)\n",
    "\n",
    "        n_way = torch.unique(support_labels).size(0)\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way)\n",
    "\n",
    "        logits = classify_queries(prototypes, query_embeddings)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        total_correct += (preds == query_labels).sum().item()\n",
    "        total_queries += query_labels.size(0)\n",
    "\n",
    "acc = total_correct / total_queries * 100\n",
    "print(f\"1-shot Accuracy on Unseen Classes (8 & 9) on Resnet18: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ef2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=CNN()\n",
    "model=ResNet50Embedding()\n",
    "##Need to ask\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad=False\n",
    "device=torch.device(\"cuda\")\n",
    "model=model.to(device)\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(),lr=1e-4)\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "epochs=20\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    total_correct=0\n",
    "    total_queries=0\n",
    "\n",
    "    for episode in dataloader:\n",
    "        support_images,support_labels, query_images, query_labels=episode\n",
    "\n",
    "        support_images=(support_images.squeeze(0)).to(device)\n",
    "        query_images=(query_images.squeeze(0)).to(device)\n",
    "        support_labels=(support_labels.view(-1)).to(device)\n",
    "        query_labels=(query_labels.view(-1)).to(device)\n",
    "\n",
    "        support_embeddings=model(support_images)\n",
    "        query_embeddings=model(query_images)\n",
    "\n",
    "        n_way=torch.unique(support_labels).size(0)\n",
    "        prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "\n",
    "        logits=classify_queries(prototypes, query_embeddings)\n",
    "        loss=loss_fn(logits, query_labels)\n",
    "        total_loss+=loss.item()\n",
    "\n",
    "        preds=torch.argmax(logits,dim=1)\n",
    "        correct=(preds==query_labels).sum().item()\n",
    "        total_correct+=correct\n",
    "        total_queries+=query_labels.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # del support_images, query_images, support_labels, query_labels\n",
    "        # del support_embeddings, query_embeddings, prototypes, logits, preds, loss\n",
    "        # torch.cuda.empty_cache()\n",
    "        # import gc\n",
    "        # gc.collect()\n",
    "\n",
    "    avg_loss=total_loss/len(dataloader)\n",
    "    acc=total_correct/total_queries *100\n",
    "\n",
    "    print(f\"Epoch:[{epoch+1}/{epochs}] --> Loss:{avg_loss} and Accuracy:{acc}\")\n",
    "\n",
    "few_dataset=fewshot_dataset(\n",
    "    data=test_data,\n",
    "    way=2,\n",
    "    shot=1,\n",
    "    query=2,\n",
    "    episodes=100\n",
    ")\n",
    "\n",
    "test_loader=DataLoader(few_dataset,batch_size=1,shuffle=True)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_queries = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for episode in test_loader:\n",
    "        support_images, support_labels, query_images, query_labels = episode\n",
    "\n",
    "        support_images = support_images.squeeze(0).to(device)\n",
    "        query_images = query_images.squeeze(0).to(device)\n",
    "        support_labels = support_labels.view(-1).to(device)\n",
    "        query_labels = query_labels.view(-1).to(device)\n",
    "\n",
    "        support_embeddings = model(support_images)\n",
    "        query_embeddings = model(query_images)\n",
    "\n",
    "        n_way = torch.unique(support_labels).size(0)\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way)\n",
    "\n",
    "        logits = classify_queries(prototypes, query_embeddings)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        total_correct += (preds == query_labels).sum().item()\n",
    "        total_queries += query_labels.size(0)\n",
    "\n",
    "acc = total_correct / total_queries * 100\n",
    "print(f\"1-shot Accuracy on Unseen Classes (8 & 9) on Resnet50: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=CNN()\n",
    "model=DenseNet121Embedding()\n",
    "##Need to ask\n",
    "\n",
    "device=torch.device(\"cuda\")\n",
    "model=model.to(device)\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(),lr=1e-4)\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "epochs=20\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    total_correct=0\n",
    "    total_queries=0\n",
    "\n",
    "    for episode in dataloader:\n",
    "        support_images,support_labels, query_images, query_labels=episode\n",
    "\n",
    "        support_images=(support_images.squeeze(0)).to(device)\n",
    "        query_images=(query_images.squeeze(0)).to(device)\n",
    "        support_labels=(support_labels.view(-1)).to(device)\n",
    "        query_labels=(query_labels.view(-1)).to(device)\n",
    "\n",
    "        support_embeddings=model(support_images)\n",
    "        query_embeddings=model(query_images)\n",
    "\n",
    "        n_way=torch.unique(support_labels).size(0)\n",
    "        prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "\n",
    "        logits=classify_queries(prototypes, query_embeddings)\n",
    "        loss=loss_fn(logits, query_labels)\n",
    "        total_loss+=loss.item()\n",
    "\n",
    "        preds=torch.argmax(logits,dim=1)\n",
    "        correct=(preds==query_labels).sum().item()\n",
    "        total_correct+=correct\n",
    "        total_queries+=query_labels.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # del support_images, query_images, support_labels, query_labels\n",
    "        # del support_embeddings, query_embeddings, prototypes, logits, preds, loss\n",
    "        # torch.cuda.empty_cache()\n",
    "        # import gc\n",
    "        # gc.collect()\n",
    "\n",
    "    avg_loss=total_loss/len(dataloader)\n",
    "    acc=total_correct/total_queries *100\n",
    "\n",
    "    print(f\"Epoch:[{epoch+1}/{epochs}] --> Loss:{avg_loss} and Accuracy:{acc}\")\n",
    "\n",
    "few_dataset=fewshot_dataset(\n",
    "    data=test_data,\n",
    "    way=2,\n",
    "    shot=1,\n",
    "    query=2,\n",
    "    episodes=100\n",
    ")\n",
    "\n",
    "test_loader=DataLoader(few_dataset,batch_size=1,shuffle=True)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_queries = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for episode in test_loader:\n",
    "        support_images, support_labels, query_images, query_labels = episode\n",
    "\n",
    "        support_images = support_images.squeeze(0).to(device)\n",
    "        query_images = query_images.squeeze(0).to(device)\n",
    "        support_labels = support_labels.view(-1).to(device)\n",
    "        query_labels = query_labels.view(-1).to(device)\n",
    "\n",
    "        support_embeddings = model(support_images)\n",
    "        query_embeddings = model(query_images)\n",
    "\n",
    "        n_way = torch.unique(support_labels).size(0)\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way)\n",
    "\n",
    "        logits = classify_queries(prototypes, query_embeddings)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        total_correct += (preds == query_labels).sum().item()\n",
    "        total_queries += query_labels.size(0)\n",
    "\n",
    "acc = total_correct / total_queries * 100\n",
    "print(f\"1-shot Accuracy on Unseen Classes (8 & 9) on DenseNet121: {acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp.env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
