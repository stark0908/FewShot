{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "73ca085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch sees 8 GPUs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch sees\", torch.cuda.device_count(), \"GPUs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b5ee91f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU cores available: 80\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"CPU cores available:\", os.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c6624c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ad8488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from torchvision.datasets.folder import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5da86ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "223056ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"/home/23dcs505/data/2750\"):\n",
    "    print(\"No dataset found\")\n",
    "fulldata=ImageFolder(root='/home/23dcs505/data/2750', transform=data_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "07a019bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_len=int((0.8)*len(fulldata))\n",
    "test_len=len(fulldata)-(train_len)\n",
    "\n",
    "train_data_set,test_data_set= random_split(fulldata,[train_len, test_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d1075bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_list=[0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7d2c4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9eb9e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list=random.sample(all_list,train_class_len)\n",
    "test_list=list(range(0,10))\n",
    "strict_test_list=list(set(all_list) - set(train_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dec17bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 0, 5, 6, 3]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[1, 2, 4, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(train_list)\n",
    "print(test_list)\n",
    "print(strict_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43fe318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ways=5\n",
    "shots=5\n",
    "queries=5\n",
    "strict_ways=len(strict_test_list)\n",
    "gpu_num=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2ea19459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e1639b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25027"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_set.indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "be71adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_sorting(dataset, class_list):\n",
    "    targets = dataset.dataset.targets\n",
    "\n",
    "    indices= [i for i in dataset.indices if targets[i] in class_list]\n",
    "    return Subset(dataset.dataset, indices)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1c9800fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=class_sorting(train_data_set,train_list)\n",
    "test_data=class_sorting(test_data_set,test_list)\n",
    "strict_test_data=class_sorting(test_data,strict_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ced87a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.7248,  0.7248,  0.7248,  ..., -0.2342, -0.2342, -0.2342],\n",
       "          [ 0.7248,  0.7248,  0.7248,  ..., -0.2342, -0.2342, -0.2342],\n",
       "          [ 0.7248,  0.7248,  0.7248,  ..., -0.2342, -0.2342, -0.2342],\n",
       "          ...,\n",
       "          [ 1.8550,  1.8550,  1.8379,  ...,  0.4337,  0.3994,  0.3994],\n",
       "          [ 1.8722,  1.8722,  1.8550,  ...,  0.4851,  0.4508,  0.4508],\n",
       "          [ 1.8722,  1.8722,  1.8550,  ...,  0.4851,  0.4508,  0.4508]],\n",
       " \n",
       "         [[ 0.5903,  0.5903,  0.5903,  ..., -0.1450, -0.1450, -0.1450],\n",
       "          [ 0.5903,  0.5903,  0.5903,  ..., -0.1450, -0.1450, -0.1450],\n",
       "          [ 0.5903,  0.5903,  0.5903,  ..., -0.1450, -0.1450, -0.1450],\n",
       "          ...,\n",
       "          [ 1.4132,  1.4132,  1.4132,  ...,  0.2577,  0.2227,  0.2227],\n",
       "          [ 1.4307,  1.4307,  1.4307,  ...,  0.2927,  0.2752,  0.2752],\n",
       "          [ 1.4307,  1.4307,  1.4307,  ...,  0.2927,  0.2752,  0.2752]],\n",
       " \n",
       "         [[ 0.5834,  0.5834,  0.5659,  ...,  0.1476,  0.1651,  0.1651],\n",
       "          [ 0.5834,  0.5834,  0.5659,  ...,  0.1476,  0.1651,  0.1651],\n",
       "          [ 0.5834,  0.5834,  0.5659,  ...,  0.1476,  0.1651,  0.1651],\n",
       "          ...,\n",
       "          [ 1.2457,  1.2457,  1.2457,  ...,  0.4091,  0.3742,  0.3742],\n",
       "          [ 1.2631,  1.2631,  1.2631,  ...,  0.4439,  0.4265,  0.4265],\n",
       "          [ 1.2631,  1.2631,  1.2631,  ...,  0.4439,  0.4265,  0.4265]]]),\n",
       " 6)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f7951d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 27000\n",
       "    Root location: /home/23dcs505/data/2750\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9314ea94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18075"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "35037a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class create_dataset(Dataset):\n",
    "    def __init__(self,data,way,shot,query,episode):\n",
    "        super().__init__()\n",
    "        self.data=data\n",
    "        self.way=way\n",
    "        self.shot=shot\n",
    "        self.query=query\n",
    "        self.episode=episode\n",
    "\n",
    "        self.class_to_indices=self._build_class_index()\n",
    "        self.classes=list(self.class_to_indices.keys())\n",
    "        \n",
    "\n",
    "    def _build_class_index(self):\n",
    "        class_index={}\n",
    "\n",
    "        targets=self.data.dataset.targets\n",
    "\n",
    "        labels = [self.data.dataset.targets[i] for i in self.data.indices]\n",
    "        \n",
    "\n",
    "\n",
    "        for indexofsubset, indexoforiginal in enumerate(self.data.indices):\n",
    "            label=targets[indexoforiginal]\n",
    "            if label not in class_index:\n",
    "                class_index[label]=[]\n",
    "            class_index[label].append(indexofsubset)\n",
    "\n",
    "        return class_index\n",
    "        \n",
    "    def __len__(self):\n",
    "            return self.episode\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        #print('hello')\n",
    "\n",
    "        #print(f\"Total available classes: {len(self.classes)}, requested way: {self.way}\")\n",
    "\n",
    "\n",
    "        selected_class=random.sample(self.classes,self.way)\n",
    "\n",
    "        support_images, support_labels=[],[]\n",
    "        query_images, query_labels=[],[]\n",
    "\n",
    "\n",
    "        label_map={class_name: i for i, class_name in enumerate(selected_class)}\n",
    "\n",
    "        for class_name in selected_class:\n",
    "            all_indices_for_class=self.class_to_indices[class_name]\n",
    "\n",
    "            selected_index=random.sample(all_indices_for_class,self.shot+self.query)\n",
    "\n",
    "            support_index=selected_index[:self.shot]\n",
    "            query_index=selected_index[self.shot:]\n",
    "\n",
    "            for i in support_index:\n",
    "                image,_=self.data[i]\n",
    "                support_images.append(image)\n",
    "                support_labels.append(torch.tensor(label_map[class_name]))\n",
    "                \n",
    "            for i in query_index:\n",
    "                image,_=self.data[i]\n",
    "                query_images.append(image)\n",
    "                query_labels.append(torch.tensor(label_map[class_name]))\n",
    "            \n",
    "        return(\n",
    "            torch.stack(support_images),\n",
    "            torch.stack(support_labels),\n",
    "            torch.stack(query_images),\n",
    "            torch.stack(query_labels)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c4451cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prototypes(support_embeddings,support_labels,way):\n",
    "    embedding_dimensions=support_embeddings.size(-1)\n",
    "    prototypes=torch.zeros(way,embedding_dimensions).to(support_embeddings.device)\n",
    "\n",
    "    for c in range(way):\n",
    "        class_mask=(support_labels==c)\n",
    "        class_embeddings=support_embeddings[class_mask]\n",
    "        prototypes[c]=class_embeddings.mean(dim=0)\n",
    "    return prototypes\n",
    "\n",
    "def classify_queries(prototypes,query_embeddings):\n",
    "    n_query=query_embeddings.size(0)\n",
    "    way=prototypes.size(0)\n",
    "\n",
    "    query_exp=query_embeddings.unsqueeze(1).expand(n_query,way,-1)\n",
    "    prototypes_exp=prototypes.unsqueeze(0).expand(n_query,way,-1)\n",
    "\n",
    "    distances=torch.sum((query_exp-prototypes_exp)**2,dim=2)\n",
    "\n",
    "    logits=-distances\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "775c60ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "few_dataset=create_dataset(\n",
    "    data=train_data,\n",
    "    way=ways,\n",
    "    shot=shots,\n",
    "    query=queries,\n",
    "    episode=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "04e9bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_dataloader=DataLoader(\n",
    "    few_dataset,\n",
    "    #batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=8, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4604d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class VGGEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        vgg = models.vgg16(pretrained=True)\n",
    "        \n",
    "        self.features = vgg.features\n",
    "        self.avgpool = vgg.avgpool\n",
    "        \n",
    "        in_features = vgg.classifier[0].in_features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4096, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model= VGGEmbedding(embedding_dim=256)\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "for param in model.features[24:].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "device = torch.device(f\"cuda:{gpu_num}\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(device)\n",
    "\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(trainable_params, lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "697e03ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on class : [7, 0, 5, 6, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"training on class :\",train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2a9a29ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()  # Python garbage collection\n",
    "torch.cuda.empty_cache()  # Clear cache for current device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "de3857d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 ------------- Loss= 0.21087220745786908 Acccuracy= 94.12\n",
      "Epoch: 2 ------------- Loss= 0.11109117899221019 Acccuracy= 97.04\n",
      "Epoch: 3 ------------- Loss= 0.07731493228719558 Acccuracy= 97.78\n",
      "Epoch: 4 ------------- Loss= 0.06289379324025333 Acccuracy= 98.36\n",
      "Epoch: 5 ------------- Loss= 0.05937879223136406 Acccuracy= 98.28\n",
      "Epoch: 6 ------------- Loss= 0.034718381384473494 Acccuracy= 99.14\n",
      "Epoch: 7 ------------- Loss= 0.04964110765995429 Acccuracy= 98.61999999999999\n",
      "Epoch: 8 ------------- Loss= 0.05351178248384713 Acccuracy= 98.76\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-9f5c238d5fb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mquery_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mn_way\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mprototypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_prototypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msupport_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_way\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassify_queries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprototypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/torch/lib64/python3.6/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/torch/lib64/python3.6/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/torch/lib64/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unique_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/torch/lib64/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0msorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_counts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m         )\n\u001b[1;32m    741\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_queries= 0,0,0\n",
    "\n",
    "    for episode in few_dataloader:\n",
    "        support_images, support_labels, query_images, query_labels=episode\n",
    "        support_images=(support_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        query_images=(query_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        support_labels=(support_labels.view(-1)).to(device, non_blocking=True)\n",
    "        query_labels=(query_labels.view(-1)).to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        support_embeddings=model(support_images)\n",
    "        query_embeddings=model(query_images)\n",
    "\n",
    "        n_way=torch.unique(support_labels).size(0)\n",
    "        prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "        logits=classify_queries(prototypes,query_embeddings)\n",
    "        loss=loss_fn(logits,query_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "        preds=torch.argmax(logits,dim=1)\n",
    "        total_correct+=(preds==query_labels).sum().item()\n",
    "        total_queries+=query_labels.size(0)\n",
    "    \n",
    "    avg_loss=total_loss/len(few_dataloader)\n",
    "    accuracy=(total_correct/total_queries)*100\n",
    "    print(\"Epoch:\",epoch+1,\"-------------\",\"Loss=\",avg_loss,\"Acccuracy=\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=create_dataset(\n",
    "    data=test_data,\n",
    "    way=ways,\n",
    "    shot=shots,\n",
    "    query=queries,\n",
    "    episode=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532fa90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader=DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=True,\n",
    "    num_workers=8, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236e5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on class : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"testing on class :\",test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b5c9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss= 0.02886864752079802 Acccuracy on 10 Class = 78.52\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_correct, total_queries= 0,0\n",
    "with torch.no_grad():\n",
    "    for episode in test_dataloader:\n",
    "        support_images, support_labels, query_images, query_labels=episode\n",
    "        support_images=(support_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        query_images=(query_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        support_labels=(support_labels.view(-1)).to(device, non_blocking=True)\n",
    "        query_labels=(query_labels.view(-1)).to(device, non_blocking=True)\n",
    "\n",
    "        support_embeddings=model(support_images)\n",
    "        query_embeddings=model(query_images)\n",
    "\n",
    "        n_way=torch.unique(support_labels).size(0)\n",
    "        prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "        logits=classify_queries(prototypes,query_embeddings)\n",
    "        \n",
    "        preds=torch.argmax(logits,dim=1)\n",
    "        total_correct+=(preds==query_labels).sum().item()\n",
    "        total_queries+=query_labels.size(0)\n",
    "    \n",
    "    #avg_loss=total_loss/len(few_dataloader)\n",
    "    accuracy_0=(total_correct/total_queries)*100\n",
    "    print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of Prototypical Network on all\", len(test_list),\"Class =\",accuracy_0,\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd745aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_test_dataset=create_dataset(\n",
    "    data=strict_test_data,\n",
    "    way=strict_ways,\n",
    "    shot=shots,\n",
    "    query=queries,\n",
    "    episode=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96358b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_test_dataloader=DataLoader(\n",
    "    strict_test_dataset,\n",
    "    #batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=8, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2277cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on class : [1, 2, 3, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"testing on class :\",strict_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10aba1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 way 3 shot Loss= 0.02886864752079802 Acccuracy on 5 Class = 60.58\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_correct, total_queries= 0,0\n",
    "with torch.no_grad():\n",
    "    for episode in strict_test_dataloader:\n",
    "        support_images, support_labels, query_images, query_labels=episode\n",
    "        support_images=(support_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        query_images=(query_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        support_labels=(support_labels.view(-1)).to(device, non_blocking=True)\n",
    "        query_labels=(query_labels.view(-1)).to(device, non_blocking=True)\n",
    "\n",
    "        support_embeddings=model(support_images)\n",
    "        query_embeddings=model(query_images)\n",
    "\n",
    "        n_way=torch.unique(support_labels).size(0)\n",
    "        prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "        logits=classify_queries(prototypes,query_embeddings)\n",
    "        \n",
    "        preds=torch.argmax(logits,dim=1)\n",
    "        total_correct+=(preds==query_labels).sum().item()\n",
    "        total_queries+=query_labels.size(0)\n",
    "    \n",
    "    #avg_loss=total_loss/len(few_dataloader)\n",
    "    accuracy_1=(total_correct/total_queries)*100\n",
    "    print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of Prototypical Network on Unseen\", len(strict_test_list),\"Class =\",accuracy_1,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19bd3f",
   "metadata": {},
   "source": [
    "**Stable Protypical Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete all unused objects\n",
    "gc.collect()\n",
    "\n",
    "# Empty PyTorch CUDA cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea490bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dropblock import DropBlock2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f421c392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e466168d112848eb8d2d497d09002aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 ------------- Loss= 0.46970345072448255 Acccuracy= 83.94\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ff0c084c8e4773a0c0899903a14fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 ------------- Loss= 0.11294508117251098 Acccuracy= 97.98\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdb1314fbee445d9958c86104fe0bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 ------------- Loss= 0.07424557770602405 Acccuracy= 98.78\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7232d273b83406fa9c7014f225d81c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 ------------- Loss= 0.055583062248770146 Acccuracy= 99.16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e11d30a4b0468fa6539683e0069ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 ------------- Loss= 0.038318783300346694 Acccuracy= 99.64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce63532f6e6f40b1b3446180f798e425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 ------------- Loss= 0.030326761977048592 Acccuracy= 99.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b092015cd9f478c90384e52fafcc0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 ------------- Loss= 0.019098240821185754 Acccuracy= 99.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60ee534fdd94e919d105f63a669b7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 ------------- Loss= 0.01500507457232743 Acccuracy= 99.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a28562ebe4464f8d8e3244144a4499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 ------------- Loss= 0.012381813611900725 Acccuracy= 99.96000000000001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a4ba5ae2324cdfb6a029e49efaa243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 ------------- Loss= 0.009562951392472313 Acccuracy= 99.98\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2ba4d871694e8cb8b28b70cea9974d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 ------------- Loss= 0.006080969567829015 Acccuracy= 100.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779f6e1c871045d68c13984ce20ba2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-84922f575fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0msupport_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mquery_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mn_way\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/torch/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-84922f575fd3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/torch/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/torch/lib64/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/torch/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/torch/lib64/python3.6/site-packages/dropblock/dropblock.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# sample mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# place mask on input device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "class VGGEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, drop_prob=0.3, block_size=3):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16(pretrained=True)\n",
    "        \n",
    "        features_list = []\n",
    "        for layer in vgg.features:\n",
    "            features_list.append(layer)\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                if len(features_list) == 17 or len(features_list) == 24:\n",
    "                    features_list.append(DropBlock2D(block_size=block_size, drop_prob=drop_prob))\n",
    "\n",
    "        self.features = nn.Sequential(*features_list)\n",
    "        self.avgpool = vgg.avgpool\n",
    "        \n",
    "       \n",
    "        in_features = vgg.classifier[0].in_features\n",
    "        \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, embedding_dim) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = VGGEmbedding(embedding_dim=256, drop_prob=0.3, block_size=3)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.features[26:].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(trainable_params, lr=1e-5) # Use a smaller LR for fine-tuning\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(f\"cuda:{gpu_num}\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "epochs=20\n",
    "\n",
    "#From code for SPN\n",
    "n_times=15\n",
    "alpha=0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_queries= 0,0,0\n",
    "\n",
    "    from tqdm.notebook import tqdm\n",
    "    progress_bar=tqdm(few_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\",leave=False)\n",
    "\n",
    "\n",
    "    for episode in progress_bar:\n",
    "        support_images, support_labels, query_images, query_labels=episode\n",
    "        support_images=(support_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        query_images=(query_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        support_labels=(support_labels.view(-1)).to(device, non_blocking=True)\n",
    "        query_labels=(query_labels.view(-1)).to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        #For montecarlopass\n",
    "        \n",
    "        all_ce_losses = []\n",
    "        all_query_logits = []\n",
    "\n",
    "        for _ in range(n_times):\n",
    "            support_embeddings=model(support_images)\n",
    "            query_embeddings=model(query_images)\n",
    "\n",
    "            n_way=torch.unique(support_labels).size(0)\n",
    "            prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "            logits=classify_queries(prototypes,query_embeddings)\n",
    "\n",
    "            ce_loss=loss_fn(logits,query_labels)\n",
    "            all_ce_losses.append(ce_loss)\n",
    "            all_query_logits.append(logits)\n",
    "            \n",
    "        total_ce_loss= torch.stack(all_ce_losses).mean()\n",
    "\n",
    "        stacked_logits=torch.stack(all_query_logits)\n",
    "        stacked_probs=torch.softmax(stacked_logits,dim=-1)\n",
    "\n",
    "        true_class_probs = stacked_probs[\n",
    "            torch.arange(n_times)[:, None],\n",
    "            torch.arange(len(query_labels)),\n",
    "            query_labels\n",
    "        ]\n",
    "            \n",
    "        variance_loss=torch.std(true_class_probs,dim=0).sum()\n",
    "        total_combined_loss=total_ce_loss+alpha*variance_loss\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        total_combined_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        total_loss+=total_combined_loss.item()\n",
    "        mean_logits=stacked_logits.mean(dim=0)\n",
    "        preds=torch.argmax(mean_logits,dim=1)\n",
    "        total_correct+=(preds==query_labels).sum().item()\n",
    "        total_queries+=query_labels.size(0)\n",
    "\n",
    "        avg_acc_till=(total_correct/total_queries)*100\n",
    "        progress_bar.set_postfix(Loss=f\"{total_combined_loss.item():4f}\",Acc=f\"{avg_acc_till}&\")\n",
    "    \n",
    "    avg_loss=total_loss/len(few_dataloader)\n",
    "    accuracy=(total_correct/total_queries)*100\n",
    "    print(\"Epoch:\",epoch+1,\"-------------\",\"Loss=\",avg_loss,\"Acccuracy=\",accuracy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65030730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f43ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing........... Started.......................\n",
      "Loss= 0.006080969567829015 Acccuracy on 10 Class = 83.52000000000001\n",
      "Mean Predictive Entropy = 0.5312187671661377\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing........... Started.......................\")\n",
    "model.eval()\n",
    "total_correct, total_queries= 0,0\n",
    "with torch.no_grad():\n",
    "    for episode in test_dataloader:\n",
    "        support_images, support_labels, query_images, query_labels=episode\n",
    "        support_images=(support_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        query_images=(query_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        support_labels=(support_labels.view(-1)).to(device, non_blocking=True)\n",
    "        query_labels=(query_labels.view(-1)).to(device, non_blocking=True)\n",
    "\n",
    "        stacked_logits=[]\n",
    "\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        for _ in range(n_times):\n",
    "\n",
    "            support_embeddings=model(support_images)\n",
    "            query_embeddings=model(query_images)\n",
    "\n",
    "            n_way=torch.unique(support_labels).size(0)\n",
    "            prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "            logits=classify_queries(prototypes,query_embeddings)\n",
    "\n",
    "            stacked_logits.append(logits)\n",
    "        \n",
    "        model.eval()\n",
    "        mean_logits=torch.stack(stacked_logits).mean(dim=0)\n",
    "        preds=torch.argmax(mean_logits,dim=1)\n",
    "        total_correct+=(preds==query_labels).sum().item()\n",
    "        total_queries+=query_labels.size(0)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    #avg_loss=total_loss/len(few_dataloader)\n",
    "    accuracy_spn_0=(total_correct/total_queries)*100\n",
    "    print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of Stable Prototypical Network on all\", len(test_list),\"Class =\",accuracy_spn_0,\"%\")\n",
    "\n",
    "\n",
    "    entropy = -(torch.softmax(mean_logits, dim=1) * torch.log_softmax(mean_logits, dim=1)).sum(dim=1).mean()\n",
    "    print(\"Mean Predictive Entropy =\", entropy.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025823f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss= 0.006080969567829015 Acccuracy on 5 Class = 67.7\n",
      "Mean Predictive Entropy = 0.4826270043849945\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_correct, total_queries= 0,0\n",
    "with torch.no_grad():\n",
    "    for episode in strict_test_dataloader:\n",
    "        support_images, support_labels, query_images, query_labels=episode\n",
    "        support_images=(support_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        query_images=(query_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        support_labels=(support_labels.view(-1)).to(device, non_blocking=True)\n",
    "        query_labels=(query_labels.view(-1)).to(device, non_blocking=True)\n",
    "\n",
    "        stacked_logits=[]\n",
    "\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        for _ in range(n_times):\n",
    "\n",
    "            support_embeddings=model(support_images)\n",
    "            query_embeddings=model(query_images)\n",
    "\n",
    "            n_way=torch.unique(support_labels).size(0)\n",
    "            prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "            logits=classify_queries(prototypes,query_embeddings)\n",
    "\n",
    "            stacked_logits.append(logits)\n",
    "        \n",
    "        model.eval()\n",
    "        mean_logits=torch.stack(stacked_logits).mean(dim=0)\n",
    "        preds=torch.argmax(mean_logits,dim=1)\n",
    "        total_correct+=(preds==query_labels).sum().item()\n",
    "        total_queries+=query_labels.size(0)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    #avg_loss=total_loss/len(few_dataloader)\n",
    "    accuracy_spn_1=(total_correct/total_queries)*100\n",
    "    print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of Stable Prototypical Network on Unseen\", len(strict_test_list),\"Class =\",accuracy_spn_1,\"%\")\n",
    "\n",
    "\n",
    "    entropy = -(torch.softmax(mean_logits, dim=1) * torch.log_softmax(mean_logits, dim=1)).sum(dim=1).mean()\n",
    "    print(\"Mean Predictive Entropy =\", entropy.item())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20908d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class VGGEmbeddingEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple VGG16-based feature extractor with no dropout.\n",
    "    This will serve as the base encoder for all subsequent methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16(pretrained=True)\n",
    "        self.features = vgg.features\n",
    "        self.avgpool = vgg.avgpool\n",
    "        \n",
    "        in_features = vgg.classifier[0].in_features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4096, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "print(\"Reusable VGGEmbeddingEncoder class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c65b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((label) * torch.pow(euclidean_distance, 2) + \n",
    "                                      (1-label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n",
    "\n",
    "print(\"Siamese Network loss function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"--- Starting Siamese Network Training ---\")\n",
    "\n",
    "# 1. Initialize Model, Loss, and Optimizer\n",
    "model = VGGEmbeddingEncoder(embedding_dim=256).to(device)\n",
    "\n",
    "# Fine-tuning setup: freeze early layers, unfreeze block 5 and classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.features[24:].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(trainable_params, lr=1e-4)\n",
    "loss_fn = ContrastiveLoss().to(device)\n",
    "epochs = 20\n",
    "\n",
    "# 2. Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(few_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "    \n",
    "    for episode in progress_bar:\n",
    "        support_images, support_labels, query_images, query_labels = episode\n",
    "        \n",
    "        # Combine support and query to create a larger pool for pairing\n",
    "        all_images = torch.cat([support_images.squeeze(0), query_images.squeeze(0)], dim=0)\n",
    "        all_labels = torch.cat([support_labels.view(-1), query_labels.view(-1)], dim=0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # --- On-the-fly pair generation ---\n",
    "        # For simplicity, we'll create a small number of pairs per episode.\n",
    "        # A more advanced implementation would have a dedicated pair-based loader.\n",
    "        img1_list, img2_list, labels_list = [], [], []\n",
    "        for i in range(all_images.size(0)):\n",
    "            # Create one positive and one negative pair for each image\n",
    "            anchor_img, anchor_label = all_images[i], all_labels[i]\n",
    "            \n",
    "            # Find positive sample\n",
    "            positive_indices = (all_labels == anchor_label).nonzero(as_tuple=True)[0]\n",
    "            positive_idx = positive_indices[positive_indices != i][0]\n",
    "            \n",
    "            # Find negative sample\n",
    "            negative_indices = (all_labels != anchor_label).nonzero(as_tuple=True)[0]\n",
    "            negative_idx = random.choice(negative_indices)\n",
    "\n",
    "            # Add positive pair\n",
    "            img1_list.append(anchor_img)\n",
    "            img2_list.append(all_images[positive_idx])\n",
    "            labels_list.append(torch.tensor(1.0))\n",
    "\n",
    "            # Add negative pair\n",
    "            img1_list.append(anchor_img)\n",
    "            img2_list.append(all_images[negative_idx])\n",
    "            labels_list.append(torch.tensor(0.0))\n",
    "\n",
    "        img1 = torch.stack(img1_list).to(device)\n",
    "        img2 = torch.stack(img2_list).to(device)\n",
    "        pair_labels = torch.stack(labels_list).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output1 = model(img1)\n",
    "        output2 = model(img2)\n",
    "        \n",
    "        loss = loss_fn(output1, output2, pair_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(Loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(few_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_shots = 1\n",
    "\n",
    "\n",
    "test_dataset = create_dataset(\n",
    "    data=test_data,\n",
    "    way=ways,\n",
    "    shot=siamese_shots, \n",
    "    query=queries, \n",
    "    episode=200\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "model.eval()\n",
    "total_correct, total_queries = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for episode in test_dataloader:\n",
    "        support_images, support_labels, query_images, query_labels = episode\n",
    "\n",
    "        support_images = support_images.squeeze(0).to(device)\n",
    "        query_images = query_images.squeeze(0).to(device)\n",
    "        support_labels = support_labels.view(-1).to(device)\n",
    "        query_labels = query_labels.view(-1).to(device)\n",
    "\n",
    "        # In a Siamese test, the support set acts as the class prototypes\n",
    "        support_embeddings = model(support_images)\n",
    "        query_embeddings = model(query_images)\n",
    "        \n",
    "        # Since it's 1-shot, the embeddings are the prototypes\n",
    "        prototypes = support_embeddings\n",
    "\n",
    "        # Classify queries based on distance to support images (prototypes)\n",
    "        logits = classify_queries(prototypes, query_embeddings)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        total_correct += (preds == query_labels).sum().item()\n",
    "        total_queries += query_labels.size(0)\n",
    "\n",
    "accuracy_siamese_0 = (total_correct / total_queries) * 100\n",
    "print(ways,\"way\",shots,\"siamese_shots:\",\"Loss=\",avg_loss,\"Acccuracy of Siamese Network on all\", len(test_list),\"Class =\",accuracy_siamese_0,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ffc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_shots = 1\n",
    "\n",
    "\n",
    "strict_test_dataset = create_dataset(\n",
    "    data=strict_test_data,\n",
    "    way=ways,\n",
    "    shot=siamese_shots, \n",
    "    query=queries, \n",
    "    episode=200\n",
    ")\n",
    "strict_test_dataloader = DataLoader(strict_test_dataset, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "model.eval()\n",
    "total_correct, total_queries = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for episode in strict_test_dataloader:\n",
    "        support_images, support_labels, query_images, query_labels = episode\n",
    "\n",
    "        support_images = support_images.squeeze(0).to(device)\n",
    "        query_images = query_images.squeeze(0).to(device)\n",
    "        support_labels = support_labels.view(-1).to(device)\n",
    "        query_labels = query_labels.view(-1).to(device)\n",
    "\n",
    "        # In a Siamese test, the support set acts as the class prototypes\n",
    "        support_embeddings = model(support_images)\n",
    "        query_embeddings = model(query_images)\n",
    "        \n",
    "        # Since it's 1-shot, the embeddings are the prototypes\n",
    "        prototypes = support_embeddings\n",
    "\n",
    "        # Classify queries based on distance to support images (prototypes)\n",
    "        logits = classify_queries(prototypes, query_embeddings)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        total_correct += (preds == query_labels).sum().item()\n",
    "        total_queries += query_labels.size(0)\n",
    "\n",
    "accuracy_siamese_1 = (total_correct / total_queries) * 100\n",
    "print(ways,\"way\",shots,\"siamese_shots:\",\"Loss=\",avg_loss,\"Acccuracy of Siamese Network on Unseen\", len(strict_test_list),\"Class =\",accuracy_siamese_1,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dcb43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "\n",
    "print(\"--- Starting MAML Training ---\")\n",
    "\n",
    "\n",
    "model = VGGEmbeddingEncoder(embedding_dim=ways).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5) # Outer loop optimizer\n",
    "\n",
    "epochs = 20\n",
    "inner_update_steps = 5 # Number of gradient steps in the inner loop\n",
    "inner_lr = 0.01 # Learning rate for the inner loop adaptation\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 2. MAML Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_meta_loss = 0.0\n",
    "    total_query_acc = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(few_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "    \n",
    "    for episode in progress_bar:\n",
    "        support_images, support_labels, query_images, query_labels = episode\n",
    "        support_images, support_labels = support_images.squeeze(0).to(device), support_labels.view(-1).to(device)\n",
    "        query_images, query_labels = query_images.squeeze(0).to(device), query_labels.view(-1).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        fast_model = copy.deepcopy(model)\n",
    "        fast_optimizer = optim.SGD(fast_model.parameters(), lr=inner_lr)\n",
    "        \n",
    "        \n",
    "        for _ in range(inner_update_steps):\n",
    "            support_preds = fast_model(support_images)\n",
    "            inner_loss = loss_fn(support_preds, support_labels)\n",
    "            fast_optimizer.zero_grad()\n",
    "            inner_loss.backward(retain_graph=True)\n",
    "            fast_optimizer.step()\n",
    "        \n",
    "        \n",
    "        query_preds = fast_model(query_images)\n",
    "        meta_loss = loss_fn(query_preds, query_labels)\n",
    "        \n",
    "        \n",
    "        meta_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += meta_loss.item()\n",
    "        total_acc += (query_preds.argmax(dim=1) == query_labels).sum().item() / len(query_labels)\n",
    "        progress_bar.set_postfix(MetaLoss=f\"{meta_loss.item():.4f}\")\n",
    "\n",
    "    avg_meta_loss = total_meta_loss / len(few_dataloader)\n",
    "    accuracy = (total_query_acc / len(few_dataloader)) * 100\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Meta-Loss: {avg_meta_loss:.4f}, Query Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae261156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAML: Evaluation Loop ---\n",
    "print(\"--- Evaluating MAML ---\")\n",
    "\n",
    "\n",
    "test_dataset = create_dataset(\n",
    "    data=test_data,\n",
    "    way=ways, \n",
    "    shot=shots, \n",
    "    query=queries,\n",
    "    episode=200\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "total_correct, total_queries = 0, 0\n",
    "\n",
    "#with torch.no_grad():\n",
    "for episode in test_dataloader:\n",
    "    support_images, support_labels, query_images, query_labels = episode\n",
    "    support_images, support_labels = support_images.squeeze(0).to(device), support_labels.view(-1).to(device)\n",
    "    query_images, query_labels = query_images.squeeze(0).to(device), query_labels.view(-1).to(device)\n",
    "\n",
    "    # Create a temporary model and adapt it to the new, unseen task\n",
    "    test_fast_model = copy.deepcopy(model)\n",
    "    test_fast_model.train() # Enable gradients for adaptation\n",
    "    test_fast_optimizer = optim.SGD(test_fast_model.parameters(), lr=inner_lr)\n",
    "\n",
    "    # Adapt the model using the support set of the unseen task\n",
    "    for _ in range(inner_update_steps * 2): # Use more update steps for testing\n",
    "        support_preds = test_fast_model(support_images)\n",
    "        inner_loss = loss_fn(support_preds, support_labels)\n",
    "        test_fast_optimizer.zero_grad()\n",
    "        inner_loss.backward()\n",
    "        test_fast_optimizer.step()\n",
    "    \n",
    "    # Evaluate the adapted model on the query set\n",
    "    test_fast_model.eval()\n",
    "    query_preds = test_fast_model(query_images)\n",
    "    preds = torch.argmax(query_preds, dim=1)\n",
    "    \n",
    "    total_correct += (preds == query_labels).sum().item()\n",
    "    total_queries += query_labels.size(0)\n",
    "\n",
    "accuracy_maml_0 = (total_correct / total_queries) * 100\n",
    "print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of MAML Network on all\", len(test_list),\"Class =\",accuracy_maml_0,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAML: Evaluation Loop ---\n",
    "print(\"--- Evaluating MAML ---\")\n",
    "\n",
    "\n",
    "strict_test_dataset = create_dataset(\n",
    "    data=strict_test_data,\n",
    "    way=ways, \n",
    "    shot=shots, \n",
    "    query=queries,\n",
    "    episode=200\n",
    ")\n",
    "strict_test_dataloader = DataLoader(strict_test_dataset, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "total_correct, total_queries = 0, 0\n",
    "\n",
    "#with torch.no_grad():\n",
    "for episode in strict_test_dataloader:\n",
    "    support_images, support_labels, query_images, query_labels = episode\n",
    "    support_images, support_labels = support_images.squeeze(0).to(device), support_labels.view(-1).to(device)\n",
    "    query_images, query_labels = query_images.squeeze(0).to(device), query_labels.view(-1).to(device)\n",
    "\n",
    "    \n",
    "    test_fast_model = copy.deepcopy(model)\n",
    "    test_fast_model.train() \n",
    "    test_fast_optimizer = optim.SGD(test_fast_model.parameters(), lr=inner_lr)\n",
    "\n",
    "    \n",
    "    for _ in range(inner_update_steps * 2): \n",
    "        support_preds = test_fast_model(support_images)\n",
    "        inner_loss = loss_fn(support_preds, support_labels)\n",
    "        test_fast_optimizer.zero_grad()\n",
    "        inner_loss.backward()\n",
    "        test_fast_optimizer.step()\n",
    "    \n",
    "    # Evaluate the adapted model on the query set\n",
    "    test_fast_model.eval()\n",
    "    query_preds = test_fast_model(query_images)\n",
    "    preds = torch.argmax(query_preds, dim=1)\n",
    "    \n",
    "    total_correct += (preds == query_labels).sum().item()\n",
    "    total_queries += query_labels.size(0)\n",
    "\n",
    "accuracy_maml_1 = (total_correct / total_queries) * 100\n",
    "print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of MAML Network on Unseen\", len(strict_test_list),\"Class =\",accuracy_maml_1,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reptile: Training Loop ---\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "\n",
    "print(\"--- Starting Reptile Training ---\")\n",
    "\n",
    "model = VGGEmbeddingEncoder(embedding_dim=ways).to(device)\n",
    "\n",
    "epochs = 20\n",
    "inner_update_steps_reptile = 5 # Number of gradient steps on the task\n",
    "inner_lr_reptile = 0.01\n",
    "meta_step_size = 0.5 # How far to move the meta-model towards the task-specific weights\n",
    "loss_fn_reptile = nn.CrossEntropyLoss()\n",
    "\n",
    "# 2. Reptile Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss_reptile = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(few_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "    \n",
    "    # Store original weights before the episode batch\n",
    "    original_weights = {name: param.clone() for name, param in model.named_parameters()}\n",
    "    \n",
    "    for episode in progress_bar:\n",
    "        support_images, support_labels, query_images, query_labels = episode\n",
    "        support_images, support_labels = support_images.squeeze(0).to(device), support_labels.view(-1).to(device)\n",
    "        \n",
    "        # Create a temporary model to train on the task\n",
    "        task_model = copy.deepcopy(model)\n",
    "        task_optimizer = optim.SGD(model.parameters(), lr=inner_lr_reptile)\n",
    "\n",
    "        # Train the task_model on the support set\n",
    "        for _ in range(inner_update_steps_reptile):\n",
    "            support_preds = task_model(support_images)\n",
    "            task_loss = loss_fn_reptile(support_preds, support_labels)\n",
    "            task_optimizer.zero_grad()\n",
    "            task_loss.backward()\n",
    "            task_optimizer.step()\n",
    "        \n",
    "        total_loss_reptile += task_loss.item() # For logging purposes\n",
    "        \n",
    "        # --- Reptile Meta-Update ---\n",
    "        # Interpolate the meta-model's weights towards the task-model's new weights\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                task_param = task_model.state_dict()[name]\n",
    "                param.data.lerp_(task_param, meta_step_size)\n",
    "        \n",
    "        progress_bar.set_postfix(TaskLoss=f\"{task_loss.item():.4f}\")\n",
    "\n",
    "    avg_task_loss = total_loss_reptile / len(few_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Avg Task Loss: {avg_task_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4455ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_correct, total_queries = 0, 0\n",
    "\n",
    "#with torch.no_grad():\n",
    "for episode in test_dataloader:\n",
    "    support_images, support_labels, query_images, query_labels = episode\n",
    "    support_images, support_labels = support_images.squeeze(0).to(device), support_labels.view(-1).to(device)\n",
    "    query_images, query_labels = query_images.squeeze(0).to(device), query_labels.view(-1).to(device)\n",
    "\n",
    "    # Create a temporary model and fine-tune it on the new, unseen task\n",
    "    test_task_model = copy.deepcopy(model)\n",
    "    test_task_model.train() # Enable gradients for fine-tuning\n",
    "    test_task_optimizer = optim.SGD(test_task_model.parameters(), lr=inner_lr_reptile)\n",
    "\n",
    "    # Fine-tune the model using the support set of the unseen task\n",
    "    for _ in range(inner_update_steps_reptile * 2): # Use more update steps for testing\n",
    "        support_preds = test_task_model(support_images)\n",
    "        inner_loss = loss_fn_reptile(support_preds, support_labels)\n",
    "        test_task_optimizer.zero_grad()\n",
    "        inner_loss.backward()\n",
    "        test_task_optimizer.step()\n",
    "    \n",
    "    # Evaluate the fine-tuned model on the query set\n",
    "    test_task_model.eval()\n",
    "    query_preds = test_task_model(query_images)\n",
    "    preds = torch.argmax(query_preds, dim=1)\n",
    "    \n",
    "    total_correct += (preds == query_labels).sum().item()\n",
    "    total_queries += query_labels.size(0)\n",
    "\n",
    "accuracy_reptile_0 = (total_correct / total_queries) * 100\n",
    "print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of Reptile Network on all\", len(strict_test_list),\"Class =\",accuracy_reptile_0,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ca37b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_correct, total_queries = 0, 0\n",
    "\n",
    "#with torch.no_grad():\n",
    "for episode in strict_test_dataloader:\n",
    "    support_images, support_labels, query_images, query_labels = episode\n",
    "    support_images, support_labels = support_images.squeeze(0).to(device), support_labels.view(-1).to(device)\n",
    "    query_images, query_labels = query_images.squeeze(0).to(device), query_labels.view(-1).to(device)\n",
    "\n",
    "    # Create a temporary model and fine-tune it on the new, unseen task\n",
    "    test_task_model = copy.deepcopy(model)\n",
    "    test_task_model.train() # Enable gradients for fine-tuning\n",
    "    test_task_optimizer = optim.SGD(test_task_model.parameters(), lr=inner_lr_reptile)\n",
    "\n",
    "    # Fine-tune the model using the support set of the unseen task\n",
    "    for _ in range(inner_update_steps_reptile * 2): # Use more update steps for testing\n",
    "        support_preds = test_task_model(support_images)\n",
    "        inner_loss = loss_fn_reptile(support_preds, support_labels)\n",
    "        test_task_optimizer.zero_grad()\n",
    "        inner_loss.backward()\n",
    "        test_task_optimizer.step()\n",
    "    \n",
    "    # Evaluate the fine-tuned model on the query set\n",
    "    test_task_model.eval()\n",
    "    query_preds = test_task_model(query_images)\n",
    "    preds = torch.argmax(query_preds, dim=1)\n",
    "    \n",
    "    total_correct += (preds == query_labels).sum().item()\n",
    "    total_queries += query_labels.size(0)\n",
    "\n",
    "accuracy_reptile_1 = (total_correct / total_queries) * 100\n",
    "print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of Reptile Network on Unseen\", len(strict_test_list),\"Class =\",accuracy_reptile_1,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65100f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of Prototypical Network on all\", len(test_list),\"Class =\",accuracy_0,\"%\")\n",
    "print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of Prototypical Network on Unseen\", len(strict_test_list),\"Class =\",accuracy_1,\"%\")\n",
    "print('\\n\\n\\n')\n",
    "print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of Stable Prototypical Network on all\", len(test_list),\"Class =\",accuracy_spn_0,\"%\")\n",
    "print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of Stable Prototypical Network on Unseen\", len(strict_test_list),\"Class =\",accuracy_spn_1,\"%\")\n",
    "print('\\n\\n\\n')\n",
    "print(ways,\"way\",shots,\"siamese_shots:\",\"Loss=\",avg_loss,\"Acccuracy of Siamese Network on all\", len(test_list),\"Class =\",accuracy_siamese_0,\"%\")\n",
    "print(ways,\"way\",shots,\"siamese_shots:\",\"Loss=\",avg_loss,\"Acccuracy of Siamese Network on Unseen\", len(strict_test_list),\"Class =\",accuracy_siamese_1,\"%\")\n",
    "print('\\n\\n\\n')\n",
    "print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of MAML Network on all\", len(test_list),\"Class =\",accuracy_maml_0,\"%\")\n",
    "print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of MAML Network on Unseen\", len(strict_test_list),\"Class =\",accuracy_maml_1,\"%\")\n",
    "print('\\n\\n\\n')\n",
    "print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of Reptile Network on all\", len(strict_test_list),\"Class =\",accuracy_reptile_0,\"%\")\n",
    "print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of Reptile Network on Unseen\", len(strict_test_list),\"Class =\",accuracy_reptile_1,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615226d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/n/n/n/n\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc317b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0e3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
